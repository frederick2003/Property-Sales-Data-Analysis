{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "070811ad-6881-4977-9a76-153be6ae7ee4",
   "metadata": {},
   "source": [
    "# Web scrapping\n",
    "\n",
    "In this notebook data from the Propery Sales webpages will be scraped and loaded into csv files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f2b3d2-585f-4ed1-9339-a8487a82c6bb",
   "metadata": {},
   "source": [
    "## Import necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07a1283a-e8bb-452a-8332-c3dfe0cec277",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import bs4\n",
    "import pandas as pd\n",
    "import csv\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e8c8e3-a911-4c2c-8dda-ec3a2569545e",
   "metadata": {},
   "source": [
    "## Create helper functions.\n",
    "Lets create a number of helper functions to scrape the web data. The following functions are defined below:\n",
    "- fetch_html(url) - Takes a URL string as an input parameter and returns a BeautifulSoup object if the url is valid.\n",
    "  \n",
    "- extract_sale_date(soup) - Takes a BeautifulSoup object as a parameter and returns the the \"Sale Date\" text.\n",
    "\n",
    "  \n",
    "- parse_property_details(value, current_property) - Parses the property details field for Bedrooms, bathrooms etc.\n",
    "\n",
    "  \n",
    "-  parse_property_rows(soup,sale_date) - Takes a BeautifulSoup object and sale_date string as input parameteres. Iterates over all property sales entries and creates a list of dictionaries, where each key is a property feature (e.g bedrooms,bathrooms etc) and its value is the value for that property feature for the given property sale."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a250bd29-f63e-46f1-9fe1-f4edfb226d60",
   "metadata": {},
   "source": [
    "### Create a helper function to fetch the HTML content of a given URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d74e5e61-c085-4c21-8b6a-f1fb3290b7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_html(url):\n",
    "    # This function takes a url string as input and returns a BeautifulSoup object if successful, None if not.\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch {url}\")\n",
    "        return None\n",
    "    return bs4.BeautifulSoup(response.text, \"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30063110-5ec5-445f-9df2-8fdca4e8b0e3",
   "metadata": {},
   "source": [
    "### Create a helper function to extract property sale date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29457e9a-2609-4668-a63d-76f58a1a77c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sale_date(soup):\n",
    "    # This function will extract the \"Sale Date\" from <span class='sold'> tag.\n",
    "    sold_span = soup.find(\"span\", class_=\"sold\")\n",
    "    return sold_span.get_text(strip=True) if sold_span else None\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e2ad8e-dc71-4e8d-85b4-ab78e695bd2d",
   "metadata": {},
   "source": [
    "### Create a helper function to parse property details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9ce9338-25d0-4c9c-9744-865dda967363",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_property_details(value, current_property):\n",
    "    # Parses the property details field for Bedrooms, bathrooms etc.\n",
    "    details = value.split(\";\")\n",
    "    for item in details:\n",
    "        item = item.strip()\n",
    "        if \":\" in item:\n",
    "            sub_key, sub_value = item.split(\":\")\n",
    "            sub_key.strip()\n",
    "            sub_value.strip()\n",
    "             # Map 'Style' to 'Stories' for consistent naming\n",
    "            if sub_key == \"Style\":\n",
    "                current_property[\"Stories\"] = sub_value\n",
    "            else:\n",
    "                current_property[sub_key] = sub_value\n",
    "        else:\n",
    "            # Handle cases like \"3 Bathrooms\" or \"2 Bedrooms\"\n",
    "            parts = item.split(\" \")\n",
    "            if len(parts) >= 2:\n",
    "                number, label = parts[0], parts[1]\n",
    "                if label.startswith(\"Bathroom\"):\n",
    "                    current_property[\"Bathrooms\"] = number\n",
    "                elif label.startswith(\"Bedroom\"):\n",
    "                    current_property[\"Bedrooms\"] = number\n",
    "    return current_property\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e00882-1756-4751-a7e7-05863fe6c2c4",
   "metadata": {},
   "source": [
    "### Create helper function to parse all property rows in the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79dfd091-10e1-4e07-bcff-464001f1967f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_property_rows(soup,sale_date):\n",
    "    # Iterate over the table rows to build a list of dictionaries\n",
    "    rows = soup.find_all(\"tr\")\n",
    "    data = []\n",
    "    current_property = {}\n",
    "\n",
    "    for row in rows:\n",
    "        tds = row.find_all(\"td\")\n",
    "        if len(tds) != 2:\n",
    "            continue  # Ignore malformed rows\n",
    "\n",
    "        key = tds[0].get_text(strip=True).replace(\":\", \"\")\n",
    "        value = tds[1].get_text(strip=True)\n",
    "\n",
    "        # Expand maulti stage details for the Property Details Fields\n",
    "        if key == \"Property Details\":\n",
    "            current_property = parse_property_details(value, current_property)\n",
    "        else:\n",
    "            current_property[key] = value\n",
    "\n",
    "        # Each property ends when we reach 'First Time Buyer'\n",
    "        if key == \"First Time Buyer\":\n",
    "            if sale_date:\n",
    "                current_property[\"Sale Date\"] = sale_date\n",
    "            data.append(current_property)\n",
    "            current_property = {}\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30ed2ed-085f-4bcc-89df-a25a0a571d8e",
   "metadata": {},
   "source": [
    "### Master scraping function\n",
    "\n",
    "Lets now use all the helper functions to grab the url content and load it into a list of dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1f4045c-09e9-4697-bac5-155b1cfcf901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A master scrape function to scrape all property sales data from a given url. \n",
    "def scrape_data(url):\n",
    "    soup = fetch_html(url)\n",
    "    if not soup:\n",
    "        return []\n",
    "    sale_data = extract_sale_date(soup)\n",
    "    property_data = parse_property_rows(soup, sale_data)\n",
    "    return property_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a115371-f8bd-4748-a394-3c9edb162ec1",
   "metadata": {},
   "source": [
    "## Formatting and Saving\n",
    "\n",
    "Now that we have successfully scraped the web data. Lets convert these lists to pandas dataframes for each year. Then lets save them to a csv files so we can clean and perform analysis on our datasets in a different notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc6ecab-84ca-480a-a2c4-ed8f12383f9f",
   "metadata": {},
   "source": [
    "### Scrape 2021 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9726330-26b3-49b9-85ee-29e75329d4d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a base url for property sales data in 2021 and a list to store the dictionaries.\n",
    "base_url = \"http://mlg.ucd.ie/modules/python/assign1/property/2021-page{:02d}.html\"\n",
    "data_2021 = []\n",
    "\n",
    "# iterate through all the 2021 property sales urls and scrape their data.\n",
    "for page_num in range(1, 18):\n",
    "    # Grab current page\n",
    "    url = base_url.format(page_num)\n",
    "    # Scrape data from that URL address.\n",
    "    page_data = scrape_data(url)\n",
    "    # Add that addresses list of dictionaries to the 2021_data list.\n",
    "    data_2021.extend(page_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f691115-f462-479a-9659-3862f8a53fc4",
   "metadata": {},
   "source": [
    "### Convert to Dataframe and Save to csv Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3af2bf1c-8ef9-4f72-8173-a23f4fdcc77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the columns for our data frame.\n",
    "columns = [\"Sale Date\", \"Sale Price\", \"Location\", \"Year Built\", \"Garden\", \"Garage\",\"Type\", \"Stories\", \"Bedrooms\", \"Bathrooms\" , \"First Time Buyer\"]\n",
    "# Convert the 2021 sales data into a pandas dataframe.\n",
    "df = pd.DataFrame(data_2021, columns=columns)\n",
    "# convert the data to a csv file and save it to the data directory.\n",
    "df.to_csv(\"data/2021_property_sales_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621e4151-eb61-494d-b70c-663fcc54bd1d",
   "metadata": {},
   "source": [
    "### Scrape 2022 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "703b360d-3be9-4c2e-a8f4-727face7e0a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a base url for property sales data in 2022 and a list to store the dictionaries.\n",
    "base_url = \"http://mlg.ucd.ie/modules/python/assign1/property/2022-page{:02d}.html\"\n",
    "data_2022 = []\n",
    "\n",
    "# iterate through all the 2022 property sales urls and scrape their data.\n",
    "for page_num in range(1, 17):\n",
    "    # Grab current page\n",
    "    url = base_url.format(page_num)\n",
    "    # Scrape data from that URL address.\n",
    "    page_data = scrape_data(url)\n",
    "    # Add that addresses list of dictionaries to the 2022_data list.\n",
    "    data_2022.extend(page_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84006c4-2b4c-49ea-ad0c-d7a4b33b1497",
   "metadata": {},
   "source": [
    "### Convert to Dataframe and Save to csv Format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd3e1801-557a-41fe-af6a-0d4549ba151e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the columns for our data frame.\n",
    "columns = [\"Sale Date\", \"Sale Price\", \"Location\", \"Year Built\", \"Garden\", \"Garage\",\"Type\", \"Stories\", \"Bedrooms\", \"Bathrooms\" , \"First Time Buyer\"]\n",
    "# Convert the 2022 sales data into a pandas dataframe.\n",
    "df = pd.DataFrame(data_2022, columns=columns)\n",
    "# Convert the data to a csv file and save it to the data directory.\n",
    "df.to_csv(\"data/2022_property_sales_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0b3588-512b-4423-a8da-b23e1f8c1344",
   "metadata": {},
   "source": [
    "### Scrape 2023 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "17dce39a-506b-4efe-b277-c3c07f1f358a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a base url for property sales data in 2023 and a list to store the dictionaries.\n",
    "base_url = \"http://mlg.ucd.ie/modules/python/assign1/property/2023-page{:02d}.html\"\n",
    "data_2023 = []\n",
    "\n",
    "# iterate through all the 2023 property sales urls and scrape their data.\n",
    "for page_num in range(1, 19):\n",
    "    # Grab current page\n",
    "    url = base_url.format(page_num)\n",
    "    # Convert the 2023 sales data into a pandas dataframe.\n",
    "    page_data = scrape_data(url)\n",
    "    # Convert the data to a csv file and save it to the data directory.\n",
    "    data_2023.extend(page_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29813282-4141-4bb2-b5e2-7b9b78c3dc52",
   "metadata": {},
   "source": [
    "### Convert to Dataframe and Save to csv Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b2b712cc-9ea8-4b2d-a3b9-6fb0097b92cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the columns for our data frame.\n",
    "columns = [\"Sale Date\", \"Sale Price\", \"Location\", \"Year Built\", \"Garden\", \"Garage\",\"Type\", \"Stories\", \"Bedrooms\", \"Bathrooms\" , \"First Time Buyer\"]\n",
    "# Convert the 2023 sales data into a pandas dataframe.\n",
    "df = pd.DataFrame(data_2023, columns=columns)\n",
    "# Convert the data to a csv file and save it to the data directory.\n",
    "df.to_csv(\"data/2023_property_sales_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e05de22-ac63-49f6-b008-4efdbe37a8b0",
   "metadata": {},
   "source": [
    "### Scrape 2024 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "51636312-e51a-4115-b76f-1c889c1adb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the columns for our data frame.\n",
    "base_url = \"http://mlg.ucd.ie/modules/python/assign1/property/2024-page{:02d}.html\"\n",
    "data_2024 = []\n",
    "\n",
    "# iterate through all the 2024 property sales urls and scrape their data.\n",
    "for page_num in range(1, 24):\n",
    "    # Grab current page\n",
    "    url = base_url.format(page_num)\n",
    "    # Convert the 2024 sales data into a pandas dataframe.\n",
    "    page_data = scrape_data(url)\n",
    "    # Convert the data to a csv file and save it to the data directory.\n",
    "    data_2024.extend(page_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6417342e-4bab-4dc1-ad3e-587c8db94297",
   "metadata": {},
   "source": [
    "### Convert to Dataframe and Save to csv Format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "10e2b759-570e-47ca-9ed9-69df1cb3bf20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the columns for our data frame.\n",
    "columns = [\"Sale Date\", \"Sale Price\", \"Location\", \"Year Built\", \"Garden\", \"Garage\",\"Type\", \"Stories\", \"Bedrooms\", \"Bathrooms\" , \"First Time Buyer\"]\n",
    "# Convert the 2024 sales data into a pandas dataframe.\n",
    "df = pd.DataFrame(data_2024, columns=columns)\n",
    "# Convert the data to a csv file and save it to the data directory.\n",
    "df.to_csv(\"data/2024_property_sales_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ab0cf7-7c26-4d56-ba54-50043e11adc3",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "We now have four csv files with raw property sales data from years 2021 - 2024. These files can now be accessed from the data directory. Our Analysis notebook will load these files, clean them and perform analysis on the data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
